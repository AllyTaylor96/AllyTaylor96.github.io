<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Alasdair Taylor: Audio, Speech + Language</title><link href="https://allytaylor96.github.io/" rel="alternate"></link><link href="https://allytaylor96.github.io/feeds/all.atom.xml" rel="self"></link><id>https://allytaylor96.github.io/</id><updated>2024-02-28T17:04:00+00:00</updated><subtitle>Speech + Language ML Engineer</subtitle><entry><title>Upskilling Project: Toxicity Classifier</title><link href="https://allytaylor96.github.io/toxicity-classifier.html" rel="alternate"></link><published>2024-02-28T17:04:00+00:00</published><updated>2024-02-28T17:04:00+00:00</updated><author><name>Alasdair Taylor</name></author><id>tag:allytaylor96.github.io,2024-02-28:/toxicity-classifier.html</id><summary type="html">&lt;p&gt;Additional detail on the Toxicity Classifier upskilling project carried out in Q1 2024.&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Introduction&lt;/h3&gt;
&lt;p&gt;The motivation behind this project was to fully complete a machine learning model build from scratch to make sure I have a nice, well-formatted example &lt;em&gt;somewhere&lt;/em&gt; of my coding capabilities, and the ability to see through a project.
Before beginning, my Github repo consisted entirely of half-finished or barely-started projects, or some code from my Masters dissertation project that makes me cringe when I look back at it.&lt;/p&gt;
&lt;p&gt;After watching a Youtube video, I was reading through the comments and noticed an incredibly rude message relating to the creator's accent.
It had been widely downvoted (which may be part of how Youtube itself filters out toxic comments as it hadn't been surfaced to the top of the comment list), but it got me thinking about how there must be a way of automatically flagging toxic comments at time of writing based on the language used or sentiments expressed.&lt;/p&gt;
&lt;p&gt;This struck me as a good ML problem to look at: having a way to automatically flag text as toxic.
Therefore, &lt;a href="https://github.com/AllyTaylor96/toxicity-classifier"&gt;this linked Git repo&lt;/a&gt; was created to work on a solution.&lt;/p&gt;
&lt;h3&gt;Data&lt;/h3&gt;
&lt;p&gt;I began with looking for an appropriate dataset.
The problem could either be framed as a regression problem (ie. &lt;em&gt;how toxic is the comment?&lt;/em&gt;) or as a classification problem (ie. &lt;em&gt;is the comment toxic?&lt;/em&gt;).
After some searching, a regression approach seemed unlikely as most publically available datasets didn't give ratings of toxicity.
&lt;a href="https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/overview"&gt;One notable dataset on Kaggle&lt;/a&gt; gave 5 different areas (ie. toxic, obscene, threat, insult) a human-rated value, that models needed to try to predict.
While this sounded interesting, I wanted the project repo to be something that someone could just run and it would download the dataset for them: the Kaggle repo requires log-in to Kaggle which adds some trickiness.&lt;/p&gt;
&lt;p&gt;Therefore, I instead settled on &lt;a href="https://huggingface.co/datasets/OxAISH-AL-LLM/wiki_toxic"&gt;this dataset from HF Datasets&lt;/a&gt; which uses the same core dataset (a selection of Wikipedia comments) that have been individually labelled as either toxic or non-toxic (framing the problem as a classification problem).
That means that the model we eventually build is going to be estimating whether a comment is toxic or not, as opposed to giving a toxicity rating.&lt;/p&gt;
&lt;h3&gt;Data Prep&lt;/h3&gt;
&lt;h4&gt;Exploring our dataset&lt;/h4&gt;
&lt;p&gt;The dataset is imbalanced (which hopefully reflects the real distribution of toxicity on typical internet comments).
It contains ~200k non-toxic comments and ~22.5k toxic comments, so a roughly 1:10 imbalance between the categories.
The HF dataset contains a 'balanced' training set, which automatically splits the data distribution appropriately: the training data is split to be 50/50 toxic/non-toxic, while the validation and test set mirror the real distribution of data (~1:10).&lt;/p&gt;
&lt;p&gt;&lt;center&gt;
| &lt;strong&gt;Dataset Split&lt;/strong&gt;     | &lt;strong&gt;Non-Toxic Count&lt;/strong&gt;   | &lt;strong&gt;Toxic Count&lt;/strong&gt;   |
|:-----------------:    |:-------------------:  |:---------------:  |
|     &lt;em&gt;Training&lt;/em&gt;        |        12934          |      12934        |
|    &lt;em&gt;Validation&lt;/em&gt;       |        28624          |       3291        |
|       &lt;em&gt;Test&lt;/em&gt;          |        57735          |       6243        | 
&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;From inspection, the dataset seemed to contain several examples that were unusually long: while this wouldn't necessarily cause a huge problem, it may impact overall performance during training when comments are padded to the maximum size.
Therefore, it was decided to have a hard limit on how long in words the sentences could be (250 by default).
This hard limit only cuts ~4k comments from the overall count and sped up training significantly. &lt;/p&gt;
&lt;h3&gt;From raw text to features&lt;/h3&gt;
&lt;p&gt;A standard encoding pipeline was followed for the dataset to prepare it for our classifier.
Due to my prior positive experience with using them in a different project, &lt;a href="https://fasttext.cc/docs/en/crawl-vectors.html"&gt;FastText vectors&lt;/a&gt; were used as our main word embedding source.&lt;/p&gt;
&lt;p&gt;The general steps I followed were:
- build a vocabulary dictionary from all text in the dataset ({word: idx})
- download FastText vectors and map between the vocabulary dictionary and the embeddings ({idx: embedding})
    - if word in vocabulary dictionary but not in FastText pre-trained embeddings, create random embedding for it&lt;/p&gt;
&lt;p&gt;&lt;center&gt;
| &lt;img alt="my_image" src="/img/toxicity_data_prep.drawio.png" title="Text-to-feature Pipeline"&gt; |
|:--:|
| &lt;em&gt;Text-to-feature Pipeline&lt;/em&gt; |
&lt;/center&gt;&lt;/p&gt;
&lt;h3&gt;Model Structure&lt;/h3&gt;
&lt;p&gt;To be written...&lt;/p&gt;
&lt;h3&gt;Training&lt;/h3&gt;
&lt;h4&gt;Initial Training&lt;/h4&gt;
&lt;p&gt;To be written...&lt;/p&gt;
&lt;h4&gt;Further Training&lt;/h4&gt;
&lt;p&gt;Write about trying different techniques here, l2 reg, hyperparam tuning, changing model size...&lt;/p&gt;
&lt;h3&gt;Test Results&lt;/h3&gt;
&lt;p&gt;To be written...&lt;/p&gt;</content><category term="Projects"></category><category term="nlp"></category><category term="toxicity"></category><category term="cnn-classification"></category></entry><entry><title>The Ethics of AI Audio</title><link href="https://allytaylor96.github.io/the-ethics-of-ai-audio.html" rel="alternate"></link><published>2022-12-16T21:13:00+00:00</published><updated>2022-11-28T23:31:00+00:00</updated><author><name>Alasdair Taylor</name></author><id>tag:allytaylor96.github.io,2022-12-16:/the-ethics-of-ai-audio.html</id><summary type="html">&lt;p&gt;A discussion of the ethical implications of improving AI technologies when it comes to audio.&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Some thoughts on AI ethics&lt;/h3&gt;
&lt;p&gt;A hot-topic issue on social media at the moment is the ethical implications of AI.
This is something that is taught and discussed in AI and machine learning courses, but seems to have hit the mainstream consciousness due to the recent popularity of technologies like &lt;a href="https://openai.com/dall-e-2/"&gt;Open AI's Dall-E 2&lt;/a&gt;, &lt;a href="https://stability.ai/blog/stable-diffusion-v2-release"&gt;Stability's Stable Diffusion 2&lt;/a&gt; and even &lt;a href="https://openai.com/blog/chatgpt/"&gt;Open AI's ChatGPT&lt;/a&gt; bringing the potential of deep learning to a viral audience.
Twitter is awash with anime characters with weird hands, generic looking fantasy landscapes and incredibly convincing essays and short stories, all being produced entirely by AI at the request of a simple text prompt.&lt;/p&gt;
&lt;p&gt;All of this content has sparked discussion and protest, with &lt;a href="https://arstechnica.com/information-technology/2022/12/artstation-artists-stage-mass-protest-against-ai-generated-artwork/"&gt;artists staging protest&lt;/a&gt; against the very idea of AI generated art in their own communities.
The sentiment (in my opinion) is entirely justified: these models only work as they are trained on huge swathes of information scraped from across the web.
Taking Stable Diffusion as an example, it is a generative model trained on the &lt;a href="https://laion.ai/blog/laion-5b/"&gt;Laion-5B&lt;/a&gt; dataset, which is a scraped collection of 5.85 billion image-text pairs.
This dataset attempts to dodge the issue of copyright by saying that it only provides 'indexes to the internet' and so washes its hands of any potential problems, while Stable Diffusion itself states that there was no opt-out process for artists as the dataset 'intended to be a general representation of the language-image connection of the Internet'.
This all results in a model that produces art that is based directly on the work of artists, but without giving any hint of credit or financial payback to the artists that have made the model possible, and both the dataset provider and the model builder are refusing to take responsibility.&lt;/p&gt;
&lt;p&gt;I have no solutions to this problem, and it's been really good reading opinions from people far smarter than me on the subject.
I try to take an optimistic view - the only reason these models are good at producing images and text is due to the fact they're built on top of incredibly talented artists and writers who have produced content on the internet.
AI art does strike me as incredibly soulless - while the text ChatGPT produces can be eerily human, I personally suspect that there is just enough text on the internet now to 'steal' a good answer to any prompt or question and reproduce it as it's own.&lt;/p&gt;
&lt;h3&gt;Is a similar problem coming for audio?&lt;/h3&gt;
&lt;p&gt;All of this discourse has made me think about a potential future ethical crossroads for audio.
I wonder if there are more stumbling blocks in the way of training huge models for generating audio: it's a lot harder in terms of storage and practice to 'mass-scrape' the internet for audio the same way these models do for images and text.
For things like generative music, I doubt that the large record labels and publishers would let an generative AI model based on their artists music get far along in development at all (based on how harsh copyright laws are around playing music on Youtube, Twitch etc.).&lt;/p&gt;
&lt;p&gt;Generative audio models are also typically more compute-heavy than image-based and text-based models.
However, the pace of ML development and deep learning implies to me that in the next few years, we will have state-of-the-art generative audio models that can produce great results.&lt;/p&gt;
&lt;p&gt;One feasible example I can think of in the near future is in foley work - instead of needing to record new soundscapes of a rainforest (for example), one could go to an AI audio generator, type in 'wet humid tropical rainforest' and the generative model could produce a soundscape that matches those requirements.
There are free sources of audio (like &lt;a href="https://freesound.org/"&gt;Freesound.com&lt;/a&gt;) that have a huge amount of free recorded soundscapes and sound effects that could be exploited for machine learning.
Is this ethical to do?
Or is this a continuation of the issues that are happening in other domains?
What happens to the foley artists, soundscape designers and audio experts that have produced the work that these models could then imitate to a near-perfect degree?&lt;/p&gt;
&lt;p&gt;It's an interesting aspect of audio to think about, and it makes me think that all artists should be uniting against this kind of blanket 'scraping' of their work.
While audio isn't a field dealing with these issues at the moment, I think it's only a few years away before we're struggling with the same kinds of issues and ignorance that artists are dealing with.
In the same way that people on Twitter seem to believe that 'anyone can draw this AI stuff', it soon will become 'anyone can record this audio stuff', and the value of the knowledge and expertise people bring to their art is diminished.&lt;/p&gt;</content><category term="Blog"></category><category term="audio"></category><category term="AI"></category><category term="ethics"></category></entry><entry><title>Speech Recognition: A Pub-level Overview</title><link href="https://allytaylor96.github.io/Speech-recognition-a-pub-level-overview.html" rel="alternate"></link><published>2022-11-28T22:09:00+00:00</published><updated>2022-11-28T22:10:00+00:00</updated><author><name>Alasdair Taylor</name></author><id>tag:allytaylor96.github.io,2022-11-28:/Speech-recognition-a-pub-level-overview.html</id><summary type="html">&lt;p&gt;A non-technical description of how &lt;strong&gt;traditional&lt;/strong&gt; speech recognition works from the audio file to the decoded text.&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Introduction&lt;/h3&gt;
&lt;p&gt;I've been puzzled trying to think of a subject for the first blog post, but luckily an idea eventually struck.
Whenever I'm asked what I do in my new job and say that I work in automatic speech recognition (or ASR for short), I can see their eyes glaze over.
Typically I try to relate it to speech recognition technologies they've heard of, mentioning prominent voice assistants like Alexa and Siri or describing transcription software.
While they are typically knowledgable of such tech and the conversation typically ends there, a few people have asked exactly what that &lt;em&gt;means&lt;/em&gt; or what I &lt;em&gt;do&lt;/em&gt;. &lt;/p&gt;
&lt;p&gt;This is a harder question to answer off-the-cuff and really requires a more technical explanation than what people can be bothered listening to.
&lt;strong&gt;However&lt;/strong&gt;, it's a valuable question to know how to answer, and so I thought it would be worth trying to outline my best non-technical explanation in a short blog post that I can refer to.&lt;/p&gt;
&lt;p&gt;Speech recognition is the translation of speech-to-text, and more broadly audio-to-text.
Both audio processing and speech are complex, so it only makes sense that trying to solve this problem is &lt;strong&gt;intensely&lt;/strong&gt; complicated.&lt;/p&gt;
&lt;p&gt;So where do we start?&lt;/p&gt;
&lt;h3&gt;Raw Audio to Features&lt;/h3&gt;
&lt;p&gt;Speech for ASR is captured in an audio signal, which is itself just a sequence of digital values that best describe the analogue speech waveform.
This sequence will typically have between 16000 and 48000 values (or samples) per second for a typical audio file.
This is a large amount of information to deal with when thinking of it in data terms, and so the first step of speech recognition is typically &lt;em&gt;extracting relevant features&lt;/em&gt; from the audio file.
This sounds more complicated than it is: all we are doing is trying to cut down these huge audio sequences into a more manageable size, without losing the parts of the file that are relevant for us to parse human speech.&lt;/p&gt;
&lt;p&gt;Without getting bogged down in technical details, a good way of doing this is through &lt;em&gt;Mel-Frequency Cepstral Coefficients (or MFCCs for short)&lt;/em&gt;.
This is a way of representing audio that better represents the human auditory system, and thanks to the fact that speech has &lt;strong&gt;evolved&lt;/strong&gt; specifically to be heard by the human ear, it turns out that MFCC's are a really good way of representing speech sounds too.
However, this raises a relevant question: what is this mythical 'speech sound' we are looking for?&lt;/p&gt;
&lt;p&gt;The answer to this lies in linguistics: I am not a linguist, so will keep this part short before I embarrass myself.
Words are made up of distinct small building blocks of sound called &lt;strong&gt;phones&lt;/strong&gt; - as an example, the word 'cat' can be thought of as three distinct phones ('k', 'ae', 't'), while the word 'bat' can be thought of as sharing all but one of these phones ('b', 'ae', 't').
As these are the core differentiator in words, if we can accurately tell what phone is being made at any given point in the sequence of MFCCs, we can (hopefully) start telling apart words.&lt;/p&gt;
&lt;p&gt;Knowing all this, we want to convert the sequence of audio samples into a sequence of MFCCs that better represent these blocks of speech.
We can choose how large a time-period these MFCCs cover - a relatively standard length is ~20ms.
These become our 'features'.&lt;/p&gt;
&lt;h3&gt;Turning Features into an Acoustic Model&lt;/h3&gt;
&lt;p&gt;At this point, we have converted our huge sequence of audio samples into a more managable sequence of 20ms speech features.
However, this is the tip of the iceberg.
Let's say we have an input file of audio we are wanting to transcribe, and we convert it as above into a sequence of MFCCs and take the first one.
We need to have a system that will determine what speech sound is 'most alike' to the MFCC in the input: this is where we need to determine the &lt;strong&gt;statistically most likely&lt;/strong&gt; speech sound is given the input feature.
This is called the &lt;em&gt;acoustic&lt;/em&gt; model, and is a very important component in decoding speech.
I won't go into detail here about Hidden Markov Models and the statistical methods for doing so, but in a grand scale it involves comparing with previous examples of the speech sound in question and spitting out a probability for each.&lt;/p&gt;
&lt;p&gt;The desired output of this system tells us the most likely speech sound for every 20ms feature, giving us a sequence of the most likely speech sound in each 20ms frame of audio input!
This brings us closer and closer to decoding the file into speech.&lt;/p&gt;
&lt;h3&gt;Above the Acoustic Model - The Lexicon&lt;/h3&gt;
&lt;p&gt;We are missing some key information at this point: we have no idea what specific speech sounds make up words!
This is where something called a &lt;em&gt;lexicon&lt;/em&gt; is needed - this can be thought of as a database for a language where it displays every valid word that can be transcribed, alongside the possible combinations of speech sounds that make up that word.
This can help account for accents too - think of the word tomato being pronounced as either 'tomayto' or 'tomahto'.
One can build that into a lexicon so that both pronounciations would be accepted and transcribed as the written word tomato.&lt;/p&gt;
&lt;h3&gt;And Above the Lexicon - The Language Model&lt;/h3&gt;
&lt;p&gt;At this point you may be thinking we must have everything that we need to transcribe speech from an audio input.
We have a sequence of the most likely speech sounds, and we have a way to assemble those into words.
However, if a speech recognition system just used the acoustic model and didn't care about the actual sentences it was producing, it would be terrible.&lt;/p&gt;
&lt;p&gt;If you take your phone and ask Siri or Alexa the nonsense phrase 'Blay me a song', it will automatically assume you meant to say 'Play me a song', no matter how hard you try to pronounce the 'B' sound at the start of 'Blay'.
The reason for this is something called the &lt;em&gt;Language Model&lt;/em&gt; - this is a component of the overall speech system that tries to produce reasonable English sentences.
It looks at a wider window of the speech sounds and considers what the most likely &lt;strong&gt;sequence&lt;/strong&gt; of words is given the words around it.
The human brain will do this automatically: if you're in a bar and someone asks 'What would you like to dr[UNINTELLIGIBLE]?', you can make a fair assumption that they're asking for a drink order and not what you would like to drive, for instance.
The language model uses the same context to try and decide what a reasonable transcription would be.&lt;/p&gt;
&lt;h3&gt;Bringing It All Together&lt;/h3&gt;
&lt;p&gt;So at this point, we have a dense and complex system that balances up the most likely speech sounds detected, the most likely words they can make up and the likelihood of the overall utterance.
The final system will balance up these likelihoods with different weights assigned to each to decide the most likely transcription given the audio input.
This part is actually one of the hardest problems to solve, as it requires picking a suitable search algorithm that can quickly and accurately figure out what the most likely sequence of words is at every time step of audio input.
This is slightly too in-depth for the purposes of this post, and so we can safely assume that &lt;em&gt;an algorithm&lt;/em&gt; is used that sorts this out for us.&lt;/p&gt;
&lt;p&gt;This is the basis of traditional speech recognition: even some modern systems still use a framework similar to this.
However, a huge amount of ASR has been replaced by machine learning, deep learning and AI which replaces and transforms a lot of these 'traditional' steps.
This type of detail is best saved for another blog post.&lt;/p&gt;
&lt;p&gt;After reading this post, I hope you have a slightly better idea of what goes into speech recognition and haven't been overwhelmed.
I have attached a diagram that displays the speech recognition pipeline in full below (provided from the &lt;a href="https://subscription.packtpub.com/book/application-development/9781783287536/8/ch08lvl1sec47/understanding-speech-recognition"&gt;attached link&lt;/a&gt;) and it makes a little more sense than it would have done at the start!&lt;/p&gt;
&lt;p&gt;&lt;center&gt;
&lt;img alt="my_image" src="/img/speech_diag.png" title="ASR Pipeline"&gt;
&lt;/center&gt;&lt;/p&gt;
&lt;h3&gt;Note From Author&lt;/h3&gt;
&lt;p&gt;I have left out far more detail than I have included: speech recognition is an entire field of industry and research and it's impossible to summarize in a &amp;lt;1500 word blog post from a junior in the field.
It is a highly technical subject rooted in mathematics, statistics and linguistics. 
If the content of the post has interested you at all, there are fantastic other resources that can be accessed: I personally think &lt;a href="https://en.wikipedia.org/wiki/Speech_recognition"&gt;the Wikipedia page for speech recognition&lt;/a&gt; is a surprisingly good place for an overview for an unfamiliar reader.
For more rigour, &lt;a href="https://books.google.co.uk/books/about/Speech_and_Language_Processing.html?id=fZmj5UNK8AQC&amp;amp;source=kp_book_description&amp;amp;redir_esc=y"&gt;Jurafsky and Martin's 'Speech and Language Processing'&lt;/a&gt; can be thought of as a bible for not only speech recognition, but all fields of speech and text processing.&lt;/p&gt;</content><category term="Blog"></category><category term="Speech"></category><category term="ASR"></category></entry></feed>