<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Alasdair Taylor: Audio, Speech + Language - Projects</title><link href="https://allytaylor96.github.io/" rel="alternate"></link><link href="https://allytaylor96.github.io/feeds/projects.atom.xml" rel="self"></link><id>https://allytaylor96.github.io/</id><updated>2024-02-28T17:04:00+00:00</updated><subtitle>Speech + Language ML Engineer</subtitle><entry><title>Upskilling Project: Toxicity Classifier</title><link href="https://allytaylor96.github.io/toxicity-classifier.html" rel="alternate"></link><published>2024-02-28T17:04:00+00:00</published><updated>2024-02-28T17:04:00+00:00</updated><author><name>Alasdair Taylor</name></author><id>tag:allytaylor96.github.io,2024-02-28:/toxicity-classifier.html</id><summary type="html">&lt;p&gt;Additional detail on the Toxicity Classifier upskilling project carried out in Q1 2024.&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Introduction&lt;/h3&gt;
&lt;p&gt;The motivation behind this project was to fully complete a machine learning model build from scratch to make sure I have a nice, well-formatted example &lt;em&gt;somewhere&lt;/em&gt; of my coding capabilities, and the ability to see through a project.
Before beginning, my Github repo consisted entirely of half-finished or barely-started projects, or some code from my Masters dissertation project that makes me cringe when I look back at it.&lt;/p&gt;
&lt;p&gt;After watching a Youtube video, I was reading through the comments and noticed an incredibly rude message relating to the creator's accent.
It had been widely downvoted (which may be part of how Youtube itself filters out toxic comments as it hadn't been surfaced to the top of the comment list), but it got me thinking about how there must be a way of automatically flagging toxic comments at time of writing based on the language used or sentiments expressed.&lt;/p&gt;
&lt;p&gt;This struck me as a good ML problem to look at: having a way to automatically flag text as toxic.
Therefore, &lt;a href="https://github.com/AllyTaylor96/toxicity-classifier"&gt;this linked Git repo&lt;/a&gt; was created to work on a solution.&lt;/p&gt;
&lt;h3&gt;Data&lt;/h3&gt;
&lt;p&gt;I began with looking for an appropriate dataset.
The problem could either be framed as a regression problem (ie. &lt;em&gt;how toxic is the comment?&lt;/em&gt;) or as a classification problem (ie. &lt;em&gt;is the comment toxic?&lt;/em&gt;).
After some searching, a regression approach seemed unlikely as most publically available datasets didn't give ratings of toxicity.
&lt;a href="https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/overview"&gt;One notable dataset on Kaggle&lt;/a&gt; gave 5 different areas (ie. toxic, obscene, threat, insult) a human-rated value, that models needed to try to predict.
While this sounded interesting, I wanted the project repo to be something that someone could just run and it would download the dataset for them: the Kaggle repo requires log-in to Kaggle which adds some trickiness.&lt;/p&gt;
&lt;p&gt;Therefore, I instead settled on &lt;a href="https://huggingface.co/datasets/OxAISH-AL-LLM/wiki_toxic"&gt;this dataset from HF Datasets&lt;/a&gt; which uses the same core dataset (a selection of Wikipedia comments) that have been individually labelled as either toxic or non-toxic (framing the problem as a classification problem).
That means that the model we eventually build is going to be estimating whether a comment is toxic or not, as opposed to giving a toxicity rating.&lt;/p&gt;
&lt;h3&gt;Data Prep&lt;/h3&gt;
&lt;h4&gt;Exploring our dataset&lt;/h4&gt;
&lt;p&gt;The dataset is imbalanced (which hopefully reflects the real distribution of toxicity on typical internet comments).
It contains ~200k non-toxic comments and ~22.5k toxic comments, so a roughly 1:10 imbalance between the categories.
The HF dataset contains a 'balanced' training set, which automatically splits the data distribution appropriately: the training data is split to be 50/50 toxic/non-toxic, while the validation and test set mirror the real distribution of data (~1:10).&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style="text-align: center;"&gt;&lt;strong&gt;Dataset Split&lt;/strong&gt;&lt;/th&gt;
&lt;th style="text-align: center;"&gt;&lt;strong&gt;Non-Toxic Count&lt;/strong&gt;&lt;/th&gt;
&lt;th style="text-align: center;"&gt;&lt;strong&gt;Toxic Count&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style="text-align: center;"&gt;&lt;em&gt;Training&lt;/em&gt;&lt;/td&gt;
&lt;td style="text-align: center;"&gt;12934&lt;/td&gt;
&lt;td style="text-align: center;"&gt;12934&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: center;"&gt;&lt;em&gt;Validation&lt;/em&gt;&lt;/td&gt;
&lt;td style="text-align: center;"&gt;28624&lt;/td&gt;
&lt;td style="text-align: center;"&gt;3291&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: center;"&gt;&lt;em&gt;Test&lt;/em&gt;&lt;/td&gt;
&lt;td style="text-align: center;"&gt;57735&lt;/td&gt;
&lt;td style="text-align: center;"&gt;6243&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;From inspection, the dataset seemed to contain several examples that were unusually long: while this wouldn't necessarily cause a huge problem, it may impact overall performance during training when comments are padded to the maximum size.
Therefore, it was decided to have a hard limit on how long in words the sentences could be (250 by default).
This hard limit only cuts ~4k comments from the overall count and sped up training significantly. &lt;/p&gt;
&lt;h3&gt;From raw text to features&lt;/h3&gt;
&lt;p&gt;A standard encoding pipeline was followed for the dataset to prepare it for our classifier.
Due to my prior positive experience with using them in a different project, &lt;a href="https://fasttext.cc/docs/en/crawl-vectors.html"&gt;FastText vectors&lt;/a&gt; were used as our main word embedding source.&lt;/p&gt;
&lt;p&gt;The general steps I followed were:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;build a vocabulary dictionary from all text in the dataset &lt;em&gt;(word -&amp;gt; idx)&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;download FastText vectors and map between the vocabulary dictionary and the embeddings &lt;em&gt;(idx -&amp;gt; embedding)&lt;/em&gt;&lt;ul&gt;
&lt;li&gt;if no FastText pre-trained embedding for word, create random embedding for it&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;encode all sentences in the dataset using the &lt;em&gt;word2idx&lt;/em&gt; -&amp;gt; &lt;em&gt;idx2embedding&lt;/em&gt; pipeline&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;center&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style="text-align: center;"&gt;&lt;img alt="my_image" src="/img/toxicity_data_prep.drawio.png" title="Text-to-feature Pipeline"&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style="text-align: center;"&gt;&lt;em&gt;Text-to-feature Pipeline&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;This gives us our encoded sentences we can pass to the model in training as (&lt;em&gt;'label', 'embedding'&lt;/em&gt;) pairs.
Here, the embeddings have the size of [MaximumSentenceLength, EmbeddingSize], or [250, 300] if default settings used.
In training, these are batched, so they would really have the size of [BatchSize, MaximumSentenceLength, EmbeddingSize] when passed to the model, or [50, 250, 300] by default.&lt;/p&gt;
&lt;h3&gt;Model Structure&lt;/h3&gt;
&lt;p&gt;It was decided to use a CNN architecture for our classifier, which is typically pretty good at things like spam detection or simpler classification tasks.
They are best at at learning and capturing local patterns/trends in the input data.
It is worth noting that they aren't great at long-range learning (ie. learning patterns in text that span over a large sequence of words) in the way that a RNN or a transformer architecture would be.&lt;/p&gt;
&lt;p&gt;From my intuition, this problem is well-suited for a CNN as:
- toxic language is typically located in small bursts, which the CNN filters should be able to learn
- the variable length of comments in the dataset isn't an issue, as the CNN architecture is robust to different sequence lengths 
- most comments aren't that long, so there isn't really a need for more complex sequence processing&lt;/p&gt;
&lt;p&gt;It also has the benefit of being small and efficient to train, with less parameters than RNNs or certainly transformer architectures - this is useful as it makes it feasible for someone to download the repo and train the model themselves without needing a large amount of compute.&lt;/p&gt;
&lt;p&gt;I began with a small and simple 1D CNN architecture to begin with: the set-up code for that is highlighted below.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Simple_CNN&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Module&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot; Simple 1D CNN Classifier. &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                 &lt;span class="n"&gt;pretrained_embedding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                 &lt;span class="n"&gt;filter_sizes&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                 &lt;span class="n"&gt;num_filters&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                 &lt;span class="n"&gt;num_classes&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                 &lt;span class="n"&gt;dropout&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;

        &lt;span class="nb"&gt;super&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Simple_CNN&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;vocab_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;embed_dim&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pretrained_embedding&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;embedding&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Embedding&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;from_pretrained&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pretrained_embedding&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                                      &lt;span class="n"&gt;freeze&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="c1"&gt;# set up convolution network&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;conv1d_list&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ModuleList&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;
            &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Conv1d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;in_channels&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;embed_dim&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                      &lt;span class="n"&gt;out_channels&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;num_filters&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                      &lt;span class="n"&gt;kernel_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;filter_sizes&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
            &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;filter_sizes&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="p"&gt;])&lt;/span&gt;
        &lt;span class="c1"&gt;# connected layer and dropout&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Linear&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_filters&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;num_classes&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dropout&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Dropout&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;dropout&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3&gt;Training&lt;/h3&gt;
&lt;h4&gt;Initial Training&lt;/h4&gt;
&lt;p&gt;The models initial training parameters were estimated at sensible values:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;batch size&lt;/em&gt;: 50 examples per batch&lt;/li&gt;
&lt;li&gt;&lt;em&gt;training epochs&lt;/em&gt;: 20&lt;/li&gt;
&lt;li&gt;&lt;em&gt;CNN filter sizes&lt;/em&gt;: 3, 4, 5&lt;/li&gt;
&lt;li&gt;&lt;em&gt;CNN number of filters&lt;/em&gt;: [100, 100, 100]&lt;/li&gt;
&lt;li&gt;&lt;em&gt;CNN dropout&lt;/em&gt;: 0.5&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Learning Rate&lt;/em&gt;: 0.01 &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;After training with these settings, I checked the results.&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style="text-align: center;"&gt;&lt;em&gt;Epoch&lt;/em&gt;&lt;/th&gt;
&lt;th style="text-align: center;"&gt;&lt;em&gt;Train Loss&lt;/em&gt;&lt;/th&gt;
&lt;th style="text-align: center;"&gt;&lt;em&gt;Validation Loss&lt;/em&gt;&lt;/th&gt;
&lt;th style="text-align: center;"&gt;&lt;em&gt;Validation Accuracy&lt;/em&gt;&lt;/th&gt;
&lt;th style="text-align: center;"&gt;&lt;em&gt;Test Accuracy&lt;/em&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style="text-align: center;"&gt;1&lt;/td&gt;
&lt;td style="text-align: center;"&gt;0.674&lt;/td&gt;
&lt;td style="text-align: center;"&gt;0.651&lt;/td&gt;
&lt;td style="text-align: center;"&gt;87.24&lt;/td&gt;
&lt;td style="text-align: center;"&gt;82.97&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: center;"&gt;2&lt;/td&gt;
&lt;td style="text-align: center;"&gt;0.623&lt;/td&gt;
&lt;td style="text-align: center;"&gt;0.595&lt;/td&gt;
&lt;td style="text-align: center;"&gt;88.95&lt;/td&gt;
&lt;td style="text-align: center;"&gt;83.76&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: center;"&gt;3&lt;/td&gt;
&lt;td style="text-align: center;"&gt;0.551&lt;/td&gt;
&lt;td style="text-align: center;"&gt;0.532&lt;/td&gt;
&lt;td style="text-align: center;"&gt;89.19&lt;/td&gt;
&lt;td style="text-align: center;"&gt;82.70&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: center;"&gt;4&lt;/td&gt;
&lt;td style="text-align: center;"&gt;0.472&lt;/td&gt;
&lt;td style="text-align: center;"&gt;0.441&lt;/td&gt;
&lt;td style="text-align: center;"&gt;91.28&lt;/td&gt;
&lt;td style="text-align: center;"&gt;84.49&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: center;"&gt;5&lt;/td&gt;
&lt;td style="text-align: center;"&gt;0.402&lt;/td&gt;
&lt;td style="text-align: center;"&gt;0.374&lt;/td&gt;
&lt;td style="text-align: center;"&gt;91.86&lt;/td&gt;
&lt;td style="text-align: center;"&gt;84.73&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: center;"&gt;&lt;strong&gt;6&lt;/strong&gt;&lt;/td&gt;
&lt;td style="text-align: center;"&gt;&lt;strong&gt;0.3524&lt;/strong&gt;&lt;/td&gt;
&lt;td style="text-align: center;"&gt;&lt;strong&gt;0.3168&lt;/strong&gt;&lt;/td&gt;
&lt;td style="text-align: center;"&gt;&lt;strong&gt;92.5894&lt;/strong&gt;&lt;/td&gt;
&lt;td style="text-align: center;"&gt;&lt;strong&gt;85.3826&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: center;"&gt;7&lt;/td&gt;
&lt;td style="text-align: center;"&gt;0.318&lt;/td&gt;
&lt;td style="text-align: center;"&gt;0.302&lt;/td&gt;
&lt;td style="text-align: center;"&gt;92.00&lt;/td&gt;
&lt;td style="text-align: center;"&gt;84.16&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: center;"&gt;8&lt;/td&gt;
&lt;td style="text-align: center;"&gt;0.292&lt;/td&gt;
&lt;td style="text-align: center;"&gt;0.272&lt;/td&gt;
&lt;td style="text-align: center;"&gt;92.34&lt;/td&gt;
&lt;td style="text-align: center;"&gt;84.50&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: center;"&gt;9&lt;/td&gt;
&lt;td style="text-align: center;"&gt;0.276&lt;/td&gt;
&lt;td style="text-align: center;"&gt;0.258&lt;/td&gt;
&lt;td style="text-align: center;"&gt;92.28&lt;/td&gt;
&lt;td style="text-align: center;"&gt;84.31&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: center;"&gt;10&lt;/td&gt;
&lt;td style="text-align: center;"&gt;0.263&lt;/td&gt;
&lt;td style="text-align: center;"&gt;0.261&lt;/td&gt;
&lt;td style="text-align: center;"&gt;91.61&lt;/td&gt;
&lt;td style="text-align: center;"&gt;83.50&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: center;"&gt;11&lt;/td&gt;
&lt;td style="text-align: center;"&gt;0.255&lt;/td&gt;
&lt;td style="text-align: center;"&gt;0.253&lt;/td&gt;
&lt;td style="text-align: center;"&gt;91.66&lt;/td&gt;
&lt;td style="text-align: center;"&gt;83.49&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: center;"&gt;12&lt;/td&gt;
&lt;td style="text-align: center;"&gt;0.247&lt;/td&gt;
&lt;td style="text-align: center;"&gt;0.236&lt;/td&gt;
&lt;td style="text-align: center;"&gt;92.17&lt;/td&gt;
&lt;td style="text-align: center;"&gt;84.10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: center;"&gt;13&lt;/td&gt;
&lt;td style="text-align: center;"&gt;0.240&lt;/td&gt;
&lt;td style="text-align: center;"&gt;0.224&lt;/td&gt;
&lt;td style="text-align: center;"&gt;92.53&lt;/td&gt;
&lt;td style="text-align: center;"&gt;84.50&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: center;"&gt;14&lt;/td&gt;
&lt;td style="text-align: center;"&gt;0.238&lt;/td&gt;
&lt;td style="text-align: center;"&gt;0.236&lt;/td&gt;
&lt;td style="text-align: center;"&gt;91.87&lt;/td&gt;
&lt;td style="text-align: center;"&gt;83.70&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: center;"&gt;15&lt;/td&gt;
&lt;td style="text-align: center;"&gt;0.234&lt;/td&gt;
&lt;td style="text-align: center;"&gt;0.225&lt;/td&gt;
&lt;td style="text-align: center;"&gt;92.30&lt;/td&gt;
&lt;td style="text-align: center;"&gt;84.14&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: center;"&gt;16&lt;/td&gt;
&lt;td style="text-align: center;"&gt;0.232&lt;/td&gt;
&lt;td style="text-align: center;"&gt;0.231&lt;/td&gt;
&lt;td style="text-align: center;"&gt;91.88&lt;/td&gt;
&lt;td style="text-align: center;"&gt;83.70&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: center;"&gt;17&lt;/td&gt;
&lt;td style="text-align: center;"&gt;0.227&lt;/td&gt;
&lt;td style="text-align: center;"&gt;0.225&lt;/td&gt;
&lt;td style="text-align: center;"&gt;92.11&lt;/td&gt;
&lt;td style="text-align: center;"&gt;83.93&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: center;"&gt;18&lt;/td&gt;
&lt;td style="text-align: center;"&gt;0.224&lt;/td&gt;
&lt;td style="text-align: center;"&gt;0.223&lt;/td&gt;
&lt;td style="text-align: center;"&gt;92.12&lt;/td&gt;
&lt;td style="text-align: center;"&gt;83.93&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: center;"&gt;19&lt;/td&gt;
&lt;td style="text-align: center;"&gt;0.219&lt;/td&gt;
&lt;td style="text-align: center;"&gt;0.222&lt;/td&gt;
&lt;td style="text-align: center;"&gt;92.10&lt;/td&gt;
&lt;td style="text-align: center;"&gt;83.90&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: center;"&gt;20&lt;/td&gt;
&lt;td style="text-align: center;"&gt;0.218&lt;/td&gt;
&lt;td style="text-align: center;"&gt;0.217&lt;/td&gt;
&lt;td style="text-align: center;"&gt;92.35&lt;/td&gt;
&lt;td style="text-align: center;"&gt;84.09&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;One can see that the best performing model on our unseen test set is the model at epoch 6, with both the validation accuracy and the test accuracy topping out then at &lt;em&gt;~92%&lt;/em&gt; and &lt;em&gt;~85%&lt;/em&gt; respectively.
As the training loss does continue to decrease after epoch 6, one could guess it starts overfitting and losing it's ability to generalise well after that point.
This plateauing behaviour can be noticed when examining the respective training graphs.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style="text-align: center;"&gt;&lt;img alt="my_image" src="/img/toxic_classifier_initial_training.png" title="Initial Training"&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style="text-align: center;"&gt;&lt;em&gt;Initial Training Performance&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The pattern of the results indicates (to me anyway) that this solution could be tuned further: the model is very simple at this point, and so is the data prep that we use.
Therefore, I decided to pursue some additional threads to try and maximise the overall performance.&lt;/p&gt;
&lt;h4&gt;Further Training&lt;/h4&gt;
&lt;p&gt;Write about trying different techniques here, l2 reg, hyperparam tuning, changing model size, batch normalization... can check the Kaggle discussions board for ideas, ie. doing interesting data processing stuff to make the embeddings better, use different embeddings, character level embeddings etc. - the 27th place solution has some interesting ideas to implement&lt;/p&gt;
&lt;h3&gt;Test Results&lt;/h3&gt;
&lt;p&gt;To be written...&lt;/p&gt;</content><category term="Projects"></category><category term="nlp"></category><category term="toxicity"></category><category term="cnn-classification"></category></entry></feed>