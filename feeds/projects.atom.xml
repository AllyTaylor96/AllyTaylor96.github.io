<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Alasdair Taylor: Audio, Speech + Language - Projects</title><link href="https://allytaylor96.github.io/" rel="alternate"></link><link href="https://allytaylor96.github.io/feeds/projects.atom.xml" rel="self"></link><id>https://allytaylor96.github.io/</id><updated>2024-02-28T17:04:00+00:00</updated><subtitle>Speech + Language ML Engineer</subtitle><entry><title>Upskilling Project: Toxicity Classifier</title><link href="https://allytaylor96.github.io/toxicity-classifier.html" rel="alternate"></link><published>2024-02-28T17:04:00+00:00</published><updated>2024-02-28T17:04:00+00:00</updated><author><name>Alasdair Taylor</name></author><id>tag:allytaylor96.github.io,2024-02-28:/toxicity-classifier.html</id><summary type="html">&lt;p&gt;Additional detail on the Toxicity Classifier upskilling project carried out in Q1 2024.&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Introduction&lt;/h3&gt;
&lt;p&gt;The motivation behind this project was to fully complete a machine learning model build from scratch to make sure I have a nice, well-formatted example &lt;em&gt;somewhere&lt;/em&gt; of my coding capabilities, and the ability to see through a project.
Before beginning, my Github repo consisted entirely of half-finished or barely-started projects, or some code from my Masters dissertation project that makes me cringe when I look back at it.&lt;/p&gt;
&lt;p&gt;After watching a Youtube video, I was reading through the comments and noticed an incredibly rude message relating to the creator's accent.
It had been widely downvoted (which may be part of how Youtube itself filters out toxic comments as it hadn't been surfaced to the top of the comment list), but it got me thinking about how there must be a way of automatically flagging toxic comments at time of writing based on the language used or sentiments expressed.&lt;/p&gt;
&lt;p&gt;This struck me as a good ML problem to look at: having a way to automatically flag text as toxic.
Therefore, &lt;a href="https://github.com/AllyTaylor96/toxicity-classifier"&gt;this linked Git repo&lt;/a&gt; was created to work on a solution.&lt;/p&gt;
&lt;h3&gt;Data&lt;/h3&gt;
&lt;p&gt;I began with looking for an appropriate dataset.
The problem could either be framed as a regression problem (ie. &lt;em&gt;how toxic is the comment?&lt;/em&gt;) or as a classification problem (ie. &lt;em&gt;is the comment toxic?&lt;/em&gt;).
After some searching, a regression approach seemed unlikely as most publically available datasets didn't give ratings of toxicity.
&lt;a href="https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/overview"&gt;One notable dataset on Kaggle&lt;/a&gt; gave 5 different areas (ie. toxic, obscene, threat, insult) a human-rated value, that models needed to try to predict.
While this sounded interesting, I wanted the project repo to be something that someone could just run and it would download the dataset for them: the Kaggle repo requires log-in to Kaggle which adds some trickiness.&lt;/p&gt;
&lt;p&gt;Therefore, I instead settled on &lt;a href="https://huggingface.co/datasets/OxAISH-AL-LLM/wiki_toxic"&gt;this dataset from HF Datasets&lt;/a&gt; which uses the same core dataset (a selection of Wikipedia comments) that have been individually labelled as either toxic or non-toxic (framing the problem as a classification problem).
That means that the model we eventually build is going to be estimating whether a comment is toxic or not, as opposed to giving a toxicity rating.&lt;/p&gt;
&lt;h3&gt;Data Prep&lt;/h3&gt;
&lt;h4&gt;Exploring our dataset&lt;/h4&gt;
&lt;p&gt;The dataset is imbalanced (which hopefully reflects the real distribution of toxicity on typical internet comments).
It contains ~200k non-toxic comments and ~22.5k toxic comments, so a roughly 1:10 imbalance between the categories.
The HF dataset contains a 'balanced' training set, which automatically splits the data distribution appropriately: the training data is split to be 50/50 toxic/non-toxic, while the validation and test set mirror the real distribution of data (~1:10).&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style="text-align: center;"&gt;&lt;strong&gt;Dataset Split&lt;/strong&gt;&lt;/th&gt;
&lt;th style="text-align: center;"&gt;&lt;strong&gt;Non-Toxic Count&lt;/strong&gt;&lt;/th&gt;
&lt;th style="text-align: center;"&gt;&lt;strong&gt;Toxic Count&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style="text-align: center;"&gt;&lt;em&gt;Training&lt;/em&gt;&lt;/td&gt;
&lt;td style="text-align: center;"&gt;12934&lt;/td&gt;
&lt;td style="text-align: center;"&gt;12934&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: center;"&gt;&lt;em&gt;Validation&lt;/em&gt;&lt;/td&gt;
&lt;td style="text-align: center;"&gt;28624&lt;/td&gt;
&lt;td style="text-align: center;"&gt;3291&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: center;"&gt;&lt;em&gt;Test&lt;/em&gt;&lt;/td&gt;
&lt;td style="text-align: center;"&gt;57735&lt;/td&gt;
&lt;td style="text-align: center;"&gt;6243&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;From inspection, the dataset seemed to contain several examples that were unusually long: while this wouldn't necessarily cause a huge problem, it may impact overall performance during training when comments are padded to the maximum size.
Therefore, it was decided to have a hard limit on how long in words the sentences could be (250 by default).
This hard limit only cuts ~4k comments from the overall count and sped up training significantly. &lt;/p&gt;
&lt;h3&gt;From raw text to features&lt;/h3&gt;
&lt;p&gt;A standard encoding pipeline was followed for the dataset to prepare it for our classifier.
Due to my prior positive experience with using them in a different project, &lt;a href="https://fasttext.cc/docs/en/crawl-vectors.html"&gt;FastText vectors&lt;/a&gt; were used as our main word embedding source.&lt;/p&gt;
&lt;p&gt;The general steps I followed were:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;build a vocabulary dictionary from all text in the dataset ({word: idx})&lt;/li&gt;
&lt;li&gt;download FastText vectors and map between the vocabulary dictionary and the embeddings ({idx: embedding})&lt;/li&gt;
&lt;li&gt;if word in vocabulary dictionary but not in FastText pre-trained embeddings, create random embedding for it&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;center&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style="text-align: center;"&gt;&lt;img alt="my_image" src="/img/toxicity_data_prep.drawio.png" title="Text-to-feature Pipeline"&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style="text-align: center;"&gt;&lt;em&gt;Text-to-feature Pipeline&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;/center&gt;&lt;/p&gt;
&lt;h3&gt;Model Structure&lt;/h3&gt;
&lt;p&gt;To be written...&lt;/p&gt;
&lt;h3&gt;Training&lt;/h3&gt;
&lt;h4&gt;Initial Training&lt;/h4&gt;
&lt;p&gt;To be written...&lt;/p&gt;
&lt;h4&gt;Further Training&lt;/h4&gt;
&lt;p&gt;Write about trying different techniques here, l2 reg, hyperparam tuning, changing model size...&lt;/p&gt;
&lt;h3&gt;Test Results&lt;/h3&gt;
&lt;p&gt;To be written...&lt;/p&gt;</content><category term="Projects"></category><category term="nlp"></category><category term="toxicity"></category><category term="cnn-classification"></category></entry></feed>