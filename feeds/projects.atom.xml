<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Alasdair Taylor: Audio, Speech + Language - Projects</title><link href="https://allytaylor96.github.io/" rel="alternate"></link><link href="https://allytaylor96.github.io/feeds/projects.atom.xml" rel="self"></link><id>https://allytaylor96.github.io/</id><updated>2024-02-28T17:04:00+00:00</updated><subtitle>Speech + Language ML Engineer</subtitle><entry><title>Upskilling Project: Toxicity Classifier</title><link href="https://allytaylor96.github.io/toxicity-classifier.html" rel="alternate"></link><published>2024-02-28T17:04:00+00:00</published><updated>2024-02-28T17:04:00+00:00</updated><author><name>Alasdair Taylor</name></author><id>tag:allytaylor96.github.io,2024-02-28:/toxicity-classifier.html</id><summary type="html">&lt;p&gt;Additional detail on the Toxicity Classifier upskilling project carried out in Q1 2024.&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Introduction&lt;/h3&gt;
&lt;p&gt;The motivation behind this project was to fully complete a machine learning model build from scratch to make sure I have a nice, well-formatted example &lt;em&gt;somewhere&lt;/em&gt; of my coding capabilities, and the ability to see through a project.
Before beginning, my Github repo consisted entirely of half-finished or barely-started projects, or some code from my Masters dissertation project that makes me cringe when I look back at it.&lt;/p&gt;
&lt;p&gt;After watching a Youtube video, I was reading through the comments and noticed an incredibly rude message relating to the creator's accent.
It had been widely downvoted (which may be part of how Youtube itself filters out toxic comments as it hadn't been surfaced to the top of the comment list), but it got me thinking about how there must be a way of automatically flagging toxic comments at time of writing based on the language used or sentiments expressed.&lt;/p&gt;
&lt;p&gt;This struck me as a good ML problem to look at: having a way to automatically flag text as toxic.
Therefore, &lt;a href="https://github.com/AllyTaylor96/toxicity-classifier"&gt;this linked Git repo&lt;/a&gt; was created to work on a solution.&lt;/p&gt;
&lt;h3&gt;Data&lt;/h3&gt;
&lt;p&gt;I began with looking for an appropriate dataset.
The problem could either be framed as a regression problem (ie. &lt;em&gt;how toxic is the comment?&lt;/em&gt;) or as a classification problem (ie. &lt;em&gt;is the comment toxic?&lt;/em&gt;).
After some searching, a regression approach seemed unlikely as most publically available datasets didn't give ratings of toxicity.
&lt;a href="https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/overview"&gt;One notable dataset on Kaggle&lt;/a&gt; gave 5 different areas (ie. toxic, obscene, threat, insult) a human-rated value, that models needed to try to predict.
While this sounded interesting, I wanted the project repo to be something that someone could just run and it would download the dataset for them: the Kaggle repo requires log-in to Kaggle which adds some trickiness.&lt;/p&gt;
&lt;p&gt;Therefore, I instead settled on &lt;a href="https://huggingface.co/datasets/OxAISH-AL-LLM/wiki_toxic"&gt;this dataset from HF Datasets&lt;/a&gt; which uses the same core dataset (a selection of Wikipedia comments) that have been individually labelled as either toxic or non-toxic (framing the problem as a classification problem).
That means that the model we eventually build is going to be estimating whether a comment is toxic or not.&lt;/p&gt;
&lt;h3&gt;Data Prep&lt;/h3&gt;
&lt;p&gt;To be written...&lt;/p&gt;
&lt;h3&gt;Model Structure&lt;/h3&gt;
&lt;p&gt;To be written...&lt;/p&gt;
&lt;h3&gt;Training&lt;/h3&gt;
&lt;h4&gt;Initial Training&lt;/h4&gt;
&lt;p&gt;To be written...&lt;/p&gt;
&lt;h4&gt;Further Training&lt;/h4&gt;
&lt;p&gt;Write about trying different techniques here, l2 reg, hyperparam tuning, changing model size...&lt;/p&gt;
&lt;h3&gt;Test Results&lt;/h3&gt;
&lt;p&gt;To be written...&lt;/p&gt;</content><category term="Projects"></category><category term="nlp"></category><category term="toxicity"></category><category term="cnn-classification"></category></entry></feed>